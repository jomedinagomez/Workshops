{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to use functions with a knowledge base\n",
    "\n",
    "This notebook builds on the concepts in the [argument generation](How_to_call_functions_with_chat_models.ipynb) notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n",
    "\n",
    "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
    "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
    "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
    "\n",
    "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "This cookbook takes you through the following workflow:\n",
    "\n",
    "- **Search utilities:** Creating the two functions that access arXiv for answers.\n",
    "- **Configure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\n",
    "- **arXiv conversation:** Put all of this together in live conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install arxiv --quiet\n",
    "!pip install PyPDF2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from openai import OpenAI,AzureOpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "GPT_MODEL = \"gpt4o\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), \n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "## Search utilities\n",
    "\n",
    "We'll first set up some utilities that will underpin our two functions.\n",
    "\n",
    "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```summarize_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data/papers' already exists.\n"
     ]
    }
   ],
   "source": [
    "directory = './data/papers'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(directory):\n",
    "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully.\")\n",
    "else:\n",
    "    # If the directory already exists, print a message indicating it\n",
    "    print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5cb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query = \"quantum\",\n",
    "        max_results = 10,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in client.results(search):\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response.data[0].embedding,\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda02bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Gauge-invariant optical selection rules for excitons',\n",
       " 'summary': 'Presently, the optical selection rules for excitons under\\ncircularly-polarized light are manifestly gauge-dependent. Recently, Fernando\\net al. introduced a gauge-invariant, quantized interband index. This index may\\nimprove the topological classification of material excited states because it is\\nintrinsically an inter-level quantity. In this work, we expand on this index to\\nintroduce its chiral formulation, and use it to make the selection rules\\ngauge-invariant. We anticipate this development to strengthen the theory of\\nquantum materials, especially two-dimensional semiconductor photophysics.',\n",
       " 'article_url': 'http://dx.doi.org/10.1016/j.physb.2024.416402',\n",
       " 'pdf_url': 'http://arxiv.org/abs/2408.08311v1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c715f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Core Argument\n",
      "- The paper introduces Decoded Quantum Interferometry (DQI), a novel quantum algorithm designed to solve classical optimization problems by transforming them into classical decoding problems.\n",
      "- DQI leverages the structure in the Fourier spectrum of the objective function to achieve this transformation, making it particularly effective for sparse max-XORSAT problems.\n",
      "- The algorithm is distinct from other quantum optimization methods like adiabatic optimization and the Quantum Approximate Optimization Algorithm (QAOA).\n",
      "\n",
      "### Evidence\n",
      "- **Algorithm Overview**:\n",
      "  - DQI uses the Quantum Fourier Transform to create interference patterns that enhance the probability of obtaining high-quality solutions.\n",
      "  - The algorithm involves state preparation, uncomputation, and applying a Hadamard transform to bias sampling towards solutions with large objective values.\n",
      "- **Benchmarking and Performance**:\n",
      "  - Initial benchmarks show that DQI with Belief Propagation (BP) decoding achieves a better approximation ratio on average for certain max-XORSAT instances compared to simulated annealing.\n",
      "  - DQI does not outperform specialized classical algorithms tailored to those instances.\n",
      "- **Exponential Quantum Speedup**:\n",
      "  - For a combinatorial optimization problem involving polynomial intersections, DQI achieves a better approximation ratio than any known polynomial-time classical algorithm, indicating an exponential quantum speedup.\n",
      "  - The paper demonstrates that a problem defined by Yamakawa and Zhandry, which shows an exponential separation between quantum and classical query complexity, is a special case efficiently solved by DQI.\n",
      "- **Technical Details**:\n",
      "  - The max-XORSAT problem is used to illustrate the DQI algorithm, with the objective function expressed to reveal its sparsity in the Fourier spectrum.\n",
      "  - The algorithm involves preparing a superposition state, applying the Hadamard transform, and measuring the state to obtain a biased sample.\n",
      "  - The uncomputation step involves solving an underdetermined linear system with a Hamming weight constraint, equivalent to decoding a classical error-correcting code.\n",
      "- **Optimization Problem Examples**:\n",
      "  - **Optimal Polynomial Intersection (OPI)**: An algebraically structured optimization problem where DQI outperforms all known classical algorithms.\n",
      "  - **Sparse max-XORSAT**: A less structured problem that may be more representative of generic industrial optimization problems.\n",
      "\n",
      "### Conclusions\n",
      "- DQI represents a new approach to quantum optimization, distinct from Hamiltonian-based methods like adiabatic optimization and QAOA.\n",
      "- The algorithm's ability to exploit sparsity in the Fourier spectrum of objective functions offers a promising avenue for achieving quantum speedup in combinatorial optimization problems.\n",
      "- The results suggest that quantum computers can potentially offer exponential speedup for certain classes of optimization problems, particularly those with specific structural properties in their Fourier spectra.\n",
      "- Further research and development of DQI could lead to practical quantum algorithms for a broader range of industrial and scientific applications.\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab07e98",
   "metadata": {},
   "source": [
    "## Configure Agent\n",
    "\n",
    "We'll create our agent in this step, including a ```Conversation``` class to support multiple turns with the API, and some Python functions to enable interaction between the ```ChatCompletion``` API and our knowledge base functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a6fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f7672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "978b7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c88ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.choices[0]\n",
    "    if full_message.finish_reason == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message.message.function_call.name == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message.message.function_call.arguments\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message.message.function_call.name,\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif (\n",
    "        full_message.message.function_call.name == \"read_article_and_summarize\"\n",
    "    ):\n",
    "        parsed_output = json.loads(\n",
    "            full_message.message.function_call.arguments\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "## arXiv conversation\n",
    "\n",
    "Let's put this all together by testing our functions out in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Core Argument\n",
       "- The paper presents Decoded Quantum Interferometry (DQI) as a novel quantum algorithm designed to transform classical optimization problems into classical decoding problems, leveraging the structure in the Fourier spectrum of the objective function.\n",
       "- DQI is particularly effective for problems like sparse max-XORSAT and Optimal Polynomial Intersection (OPI), demonstrating potential quantum speedup over classical algorithms.\n",
       "\n",
       "### Evidence\n",
       "- **Comparison with Classical Algorithms**:\n",
       "  - DQI with belief propagation (BP) decoding shows better average approximation ratios for certain max-XORSAT instances compared to simulated annealing.\n",
       "  - However, specialized classical algorithms tailored to those instances can still outperform DQI.\n",
       "- **Exponential Quantum Speedup**:\n",
       "  - For a combinatorial optimization problem involving finding polynomials that intersect the maximum number of points, DQI achieves a better approximation ratio than any known polynomial-time classical algorithm, indicating an exponential quantum speedup.\n",
       "- **Special Case Analysis**:\n",
       "  - The problem defined by Yamakawa and Zhandry, which demonstrates an exponential separation between quantum and classical query complexity, is a special case of the optimization problem efficiently solved by DQI.\n",
       "- **Algorithm Mechanics**:\n",
       "  - DQI uses the Quantum Fourier Transform to enhance the probability of obtaining good solutions by constructive interference on symbol strings with large objective values.\n",
       "  - The algorithm exploits sparsity in the Fourier spectrum of the objective functions and can handle more complex structures if present.\n",
       "- **Example Applications**:\n",
       "  - **Max-XORSAT**: DQI prepares a state that biases the sampling towards solutions with higher objective values, improving the likelihood of finding optimal solutions.\n",
       "  - **Optimal Polynomial Intersection (OPI)**: DQI outperforms all known classical algorithms for OPI, demonstrating its effectiveness.\n",
       "\n",
       "### Conclusions\n",
       "- **Performance and Potential**:\n",
       "  - DQI represents a novel approach to quantum optimization, leveraging interference patterns rather than Hamiltonians.\n",
       "  - The algorithm shows promise in achieving better results than classical algorithms for specific problem classes, particularly those with sparse Fourier spectra.\n",
       "- **Implications for Quantum Computing**:\n",
       "  - The research suggests potential for quantum computers to achieve exponential speedup in certain combinatorial optimization problems, addressing a longstanding question in the field.\n",
       "  - Further exploration and refinement of DQI could lead to significant advancements in quantum optimization techniques.\n",
       "- **Future Research Directions**:\n",
       "  - Identifying additional classes of problems where DQI outperforms any polynomial-time classical algorithm, especially algebraically unstructured problems.\n",
       "  - Testing against a broader array of classical algorithms beyond the four considered (simulated annealing, greedy search, truncation heuristic, and AdvRand).\n",
       "  - Exploring different families of optimization problems, decoders, and classical optimization algorithms.\n",
       "\n",
       "In summary, the paper highlights the potential of DQI in solving specific optimization problems more efficiently than classical methods, particularly in cases where the problem structure allows for quantum speedup. The findings suggest promising directions for future research in quantum optimization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message.content\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0fdec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
